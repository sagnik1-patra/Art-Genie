{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf56d40b-c3a7-4cdc-b40a-576c13e60252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 11:05:46,234 | INFO | Reading CSV: C:\\Users\\sagni\\Downloads\\Art Genie\\archive\\wikiart_scraped.csv\n",
      "2025-08-31 11:05:47,070 | INFO | Using label column: 'style'\n",
      "2025-08-31 11:05:47,071 | INFO | Using text columns: ['style', 'artist']\n",
      "2025-08-31 11:05:47,726 | INFO | Filtered 46 rows due to rare classes (<8). Remaining classes: 201\n",
      "2025-08-31 11:05:47,755 | INFO | Num classes: 201\n",
      "2025-08-31 11:05:47,802 | INFO | [SKLEARN] Fitting TF-IDF + LogisticRegression...\n",
      "2025-08-31 11:06:28,876 | INFO | [SKLEARN] Saved → C:\\Users\\sagni\\Downloads\\Art Genie\\artgenie_textclf.pkl\n",
      "2025-08-31 11:06:28,877 | INFO | [KERAS] Building & training text classifier...\n",
      "2025-08-31 11:10:42,074 | WARNING | You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-08-31 11:10:42,337 | INFO | [KERAS] Saved → C:\\Users\\sagni\\Downloads\\Art Genie\\artgenie_textclf.h5\n",
      "2025-08-31 11:10:42,501 | INFO | [META] Saved sample predictions → C:\\Users\\sagni\\Downloads\\Art Genie\\sample_predictions.csv\n",
      "2025-08-31 11:10:42,503 | INFO | [META] Saved label mapping → C:\\Users\\sagni\\Downloads\\Art Genie\\label_mapping.json\n",
      "2025-08-31 11:10:42,503 | INFO | [META] Saved metrics → C:\\Users\\sagni\\Downloads\\Art Genie\\metrics.json\n",
      "2025-08-31 11:10:42,507 | INFO | [META] Saved config → C:\\Users\\sagni\\Downloads\\Art Genie\\artgenie_config.yaml\n",
      "2025-08-31 11:10:42,507 | INFO | === ArtGenie Text Models Trained & Saved ===\n",
      "2025-08-31 11:10:42,508 | INFO | PKL -> C:\\Users\\sagni\\Downloads\\Art Genie\\artgenie_textclf.pkl\n",
      "2025-08-31 11:10:42,508 | INFO | H5  -> C:\\Users\\sagni\\Downloads\\Art Genie\\artgenie_textclf.h5\n",
      "2025-08-31 11:10:42,510 | INFO | YAML-> C:\\Users\\sagni\\Downloads\\Art Genie\\artgenie_config.yaml\n",
      "2025-08-31 11:10:42,510 | INFO | JSON-> C:\\Users\\sagni\\Downloads\\Art Genie\\metrics.json\n",
      "2025-08-31 11:10:42,511 | INFO | SAMPLE_PREDS -> C:\\Users\\sagni\\Downloads\\Art Genie\\sample_predictions.csv\n",
      "2025-08-31 11:10:42,511 | INFO | LABELS -> C:\\Users\\sagni\\Downloads\\Art Genie\\label_mapping.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "import warnings\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, top_k_accuracy_score\n",
    "import joblib\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "CSV_PATH     = r\"C:\\Users\\sagni\\Downloads\\Art Genie\\archive\\wikiart_scraped.csv\"\n",
    "ARTIFACT_DIR = r\"C:\\Users\\sagni\\Downloads\\Art Genie\"\n",
    "\n",
    "PKL_PATH       = os.path.join(ARTIFACT_DIR, \"artgenie_textclf.pkl\")\n",
    "H5_PATH        = os.path.join(ARTIFACT_DIR, \"artgenie_textclf.h5\")\n",
    "CONFIG_YAML    = os.path.join(ARTIFACT_DIR, \"artgenie_config.yaml\")\n",
    "METRICS_JSON   = os.path.join(ARTIFACT_DIR, \"metrics.json\")\n",
    "PRED_SAMPLE    = os.path.join(ARTIFACT_DIR, \"sample_predictions.csv\")\n",
    "LABELS_JSON    = os.path.join(ARTIFACT_DIR, \"label_mapping.json\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "POSSIBLE_TEXT_COLS = [\n",
    "    \"title\",\"description\",\"caption\",\"tags\",\"genre\",\"style\",\"artist\",\n",
    "    \"movement\",\"content\",\"about\",\"wiki\",\"text\",\"meta\",\"materials\",\"subject\"\n",
    "]\n",
    "POSSIBLE_LABEL_COLS = [\"style\",\"genre\",\"artist\"]\n",
    "\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def canonicalize(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).lower()).strip()\n",
    "\n",
    "def pick_label_column(df: pd.DataFrame) -> str:\n",
    "    for c in POSSIBLE_LABEL_COLS:\n",
    "        if c in df.columns and df[c].notna().any():\n",
    "            return c\n",
    "    if \"label\" in df.columns:\n",
    "        return \"label\"\n",
    "    raise ValueError(\"No suitable label column found (expected one of: style/genre/artist/label).\")\n",
    "\n",
    "def collect_text_columns(df: pd.DataFrame) -> List[str]:\n",
    "    cols = [c for c in POSSIBLE_TEXT_COLS if c in df.columns]\n",
    "    if not cols:\n",
    "        cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    return cols[:8]\n",
    "\n",
    "def build_text_series(df: pd.DataFrame, text_cols: List[str]) -> pd.Series:\n",
    "    if not text_cols: raise ValueError(\"No text columns found.\")\n",
    "    parts = [df[c].astype(str) for c in text_cols]\n",
    "    txt = parts[0]\n",
    "    for p in parts[1:]:\n",
    "        txt = txt.str.cat(p, sep=\" . \", na_rep=\"\")\n",
    "    return txt.fillna(\"\").map(lambda s: re.sub(r\"\\s+\", \" \", str(s)).strip())\n",
    "\n",
    "def filter_min_samples(df: pd.DataFrame, label_col: str, min_count: int = 8) -> pd.DataFrame:\n",
    "    vc = df[label_col].value_counts()\n",
    "    keep = set(vc[vc >= min_count].index)\n",
    "    filtered = df[df[label_col].isin(keep)].copy()\n",
    "    dropped = len(df) - len(filtered)\n",
    "    if dropped > 0:\n",
    "        logging.info(f\"Filtered {dropped} rows due to rare classes (<{min_count}). Remaining classes: {len(keep)}\")\n",
    "    return filtered\n",
    "\n",
    "# -----------------------------\n",
    "# Keras model (text-only)\n",
    "# -----------------------------\n",
    "def build_keras_text_model(vocab_size: int, max_len: int, num_classes: int) -> keras.Model:\n",
    "    text_inp = keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n",
    "    vectorizer = layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=max_len,\n",
    "        standardize=\"lower_and_strip_punctuation\",\n",
    "        split=\"whitespace\"\n",
    "    )\n",
    "    x = vectorizer(text_inp)\n",
    "    x = layers.Embedding(vocab_size, 128, name=\"embed\")(x)\n",
    "    x = layers.Conv1D(128, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\", name=\"label\")(x)\n",
    "\n",
    "    model = keras.Model(text_inp, out)\n",
    "    # IMPORTANT: sparse-friendly metrics\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(k=3, name=\"top3_acc\"),\n",
    "        ],\n",
    "    )\n",
    "    model.vectorizer = vectorizer\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    ensure_dir(ARTIFACT_DIR)\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "\n",
    "    logging.info(f\"Reading CSV: {CSV_PATH}\")\n",
    "    df = pd.read_csv(CSV_PATH, engine=\"python\")\n",
    "    df = normalize_cols(df)\n",
    "\n",
    "    label_col = pick_label_column(df)\n",
    "    text_cols = collect_text_columns(df)\n",
    "    logging.info(f\"Using label column: '{label_col}'\")\n",
    "    logging.info(f\"Using text columns: {text_cols}\")\n",
    "\n",
    "    df[\"__text__\"] = build_text_series(df, text_cols)\n",
    "    df[label_col] = df[label_col].astype(str).map(canonicalize)\n",
    "    df = df[(df[\"__text__\"].str.len() > 0) & (df[label_col].str.len() > 0)].copy()\n",
    "    df = filter_min_samples(df, label_col, min_count=8)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No data left after filtering. Relax min_count or check CSV.\")\n",
    "\n",
    "    X_text = df[\"__text__\"].values\n",
    "    y_raw  = df[label_col].values\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y_raw)\n",
    "    class_names = list(le.classes_)\n",
    "    num_classes = len(class_names)\n",
    "    logging.info(f\"Num classes: {num_classes}\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # 1) sklearn pipeline → PKL\n",
    "    logging.info(\"[SKLEARN] Fitting TF-IDF + LogisticRegression...\")\n",
    "    sk_pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            max_features=100_000, ngram_range=(1, 2),\n",
    "            lowercase=True, strip_accents=\"unicode\", min_df=2\n",
    "        )),\n",
    "        (\"logreg\", LogisticRegression(max_iter=1000, n_jobs=-1 if hasattr(LogisticRegression, \"n_jobs\") else None))\n",
    "    ])\n",
    "    sk_pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_prob_sk = None\n",
    "    try:\n",
    "        y_prob_sk = sk_pipe.predict_proba(X_val)\n",
    "        y_pred_sk = np.argmax(y_prob_sk, axis=1)\n",
    "    except Exception:\n",
    "        y_pred_sk = sk_pipe.predict(X_val)\n",
    "\n",
    "    acc_sk = float(accuracy_score(y_val, y_pred_sk))\n",
    "    f1_sk  = float(f1_score(y_val, y_pred_sk, average=\"weighted\"))\n",
    "    top3_sk = float(top_k_accuracy_score(y_val, y_prob_sk, k=3)) if y_prob_sk is not None else None\n",
    "\n",
    "    joblib.dump({\"pipeline\": sk_pipe, \"label_encoder\": le}, PKL_PATH)\n",
    "    logging.info(f\"[SKLEARN] Saved → {PKL_PATH}\")\n",
    "\n",
    "    # 2) Keras model → H5\n",
    "    logging.info(\"[KERAS] Building & training text classifier...\")\n",
    "    VOCAB_SIZE = 40_000\n",
    "    MAX_LEN = 128\n",
    "    keras_model = build_keras_text_model(VOCAB_SIZE, MAX_LEN, num_classes)\n",
    "\n",
    "    # adapt vectorizer\n",
    "    keras_model.vectorizer.adapt(tf.data.Dataset.from_tensor_slices(X_train).batch(256))\n",
    "\n",
    "    def make_ds(texts, labels, train=True):\n",
    "        ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "        if train: ds = ds.shuffle(8192, reshuffle_each_iteration=True)\n",
    "        return ds.batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    ds_train = make_ds(X_train, y_train, train=True)\n",
    "    ds_val   = make_ds(X_val, y_val, train=False)\n",
    "\n",
    "    es = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)\n",
    "    hist = keras_model.fit(ds_train, validation_data=ds_val, epochs=25, verbose=0, callbacks=[es])\n",
    "\n",
    "    eval_res = keras_model.evaluate(ds_val, verbose=0)\n",
    "    # eval order: [loss, accuracy, top3_acc]\n",
    "    acc_keras   = float(eval_res[1]) if len(eval_res) > 1 else None\n",
    "    top3_keras  = float(eval_res[2]) if len(eval_res) > 2 else None\n",
    "\n",
    "    keras_model.save(H5_PATH)\n",
    "    logging.info(f\"[KERAS] Saved → {H5_PATH}\")\n",
    "\n",
    "    # sample predictions\n",
    "    if y_prob_sk is not None:\n",
    "        top1_idx  = np.argmax(y_prob_sk, axis=1)\n",
    "        top1_lbl  = le.inverse_transform(top1_idx)\n",
    "        top1_conf = y_prob_sk[np.arange(len(top1_idx)), top1_idx]\n",
    "    else:\n",
    "        top1_lbl  = le.inverse_transform(y_pred_sk)\n",
    "        top1_conf = np.full_like(y_pred_sk, np.nan, dtype=float)\n",
    "\n",
    "    # keras sample preds\n",
    "    ds_val_text = tf.data.Dataset.from_tensor_slices(X_val[:200]).batch(256)\n",
    "    y_prob_k = keras_model.predict(ds_val_text, verbose=0)\n",
    "    y_idx_k  = np.argmax(y_prob_k, axis=1)\n",
    "    y_lbl_k  = le.inverse_transform(y_idx_k)\n",
    "\n",
    "    val_df = pd.DataFrame({\n",
    "        \"text\": X_val[:200],\n",
    "        \"true_label\": le.inverse_transform(y_val[:200]),\n",
    "        \"pred_sklearn\": top1_lbl[:200],\n",
    "        \"pred_sklearn_conf\": top1_conf[:200],\n",
    "        \"pred_keras\": y_lbl_k[:200],\n",
    "        \"pred_keras_conf\": y_prob_k[np.arange(len(y_idx_k[:200])), y_idx_k[:200]]\n",
    "    })\n",
    "    val_df.to_csv(PRED_SAMPLE, index=False)\n",
    "    logging.info(f\"[META] Saved sample predictions → {PRED_SAMPLE}\")\n",
    "\n",
    "    # label mapping\n",
    "    label_map = {int(i): name for i, name in enumerate(class_names)}\n",
    "    with open(LABELS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(label_map, f, indent=2, ensure_ascii=False)\n",
    "    logging.info(f\"[META] Saved label mapping → {LABELS_JSON}\")\n",
    "\n",
    "    # metrics + config\n",
    "    metrics = {\n",
    "        \"sklearn\": {\"accuracy\": acc_sk, \"f1_weighted\": f1_sk, \"top3_accuracy\": top3_sk},\n",
    "        \"keras\":   {\"accuracy\": acc_keras, \"top3_accuracy\": top3_keras, \"epochs_trained\": int(len(hist.history.get('loss', [])))},\n",
    "        \"n_classes\": int(len(class_names)),\n",
    "        \"n_rows\": int(len(df))\n",
    "    }\n",
    "    with open(METRICS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "    logging.info(f\"[META] Saved metrics → {METRICS_JSON}\")\n",
    "\n",
    "    cfg = {\n",
    "        \"data_path\": CSV_PATH,\n",
    "        \"artifact_dir\": ARTIFACT_DIR,\n",
    "        \"label_column\": label_col,\n",
    "        \"text_columns_used\": text_cols,\n",
    "        \"models\": {\n",
    "            \"sklearn_pipeline_pkl\": os.path.basename(PKL_PATH),\n",
    "            \"keras_model_h5\": os.path.basename(H5_PATH)\n",
    "        },\n",
    "        \"label_mapping_json\": os.path.basename(LABELS_JSON),\n",
    "        \"training\": {\n",
    "            \"sklearn\": {\"tfidf_max_features\": 100_000, \"ngram_range\": [1, 2]},\n",
    "            \"keras\": {\"vocab_size\": 40_000, \"max_len\": 128, \"epochs_max\": 25}\n",
    "        },\n",
    "        \"notes\": \"Text-only classifier; sparse labels with SparseTopKCategoricalAccuracy.\"\n",
    "    }\n",
    "    with open(CONFIG_YAML, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
    "    logging.info(f\"[META] Saved config → {CONFIG_YAML}\")\n",
    "\n",
    "    logging.info(\"=== ArtGenie Text Models Trained & Saved ===\")\n",
    "    logging.info(f\"PKL -> {PKL_PATH}\")\n",
    "    logging.info(f\"H5  -> {H5_PATH}\")\n",
    "    logging.info(f\"YAML-> {CONFIG_YAML}\")\n",
    "    logging.info(f\"JSON-> {METRICS_JSON}\")\n",
    "    logging.info(f\"SAMPLE_PREDS -> {PRED_SAMPLE}\")\n",
    "    logging.info(f\"LABELS -> {LABELS_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655784c-e1fc-4374-abb6-39b466861665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
